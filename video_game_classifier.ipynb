{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "85ec89a2-17e3-46bf-8296-378b2b84798c",
   "metadata": {},
   "source": [
    "# Video Game Classifier Project\n",
    "Tyler Short and Gideon Keith-Stanley\n",
    "\n",
    "### Background\n",
    "The PC Games 2020 dataset contains the results of scraping and sorting the entire catalog of Valve's \"Steam\" video game store, and includes data on over 27,000 titles. These data include title, description, genre, price points, several success metrics, and more. We hypothesize that using the \"bag of words\" method as seen in email spam filters, we can train a machine learning model using the digested description of video games, and use that information to classify games by genre."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "869f84fa-fb3e-4cb3-a472-468f0bc6b77c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8bc4dc7-e70c-4135-88a7-d4fa3b8d34b8",
   "metadata": {},
   "source": [
    "### Data Loader\n",
    "This routine downloads the dataset from OpenML.org and processes it with liac-arff. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3e8111a-8972-4ebe-a6d1-2ad22528a3d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arff\n",
    "\n",
    "from urllib.request import urlretrieve\n",
    "\n",
    "def load_game_data():\n",
    "    url = 'https://api.openml.org/data/v1/download/22102514/PC-Games-2020.arff'\n",
    "    filename = 'pc_game_dataset.arff'\n",
    "    file, http_response = urlretrieve(url, filename)\n",
    "    dataset = arff.load(open(file, 'r'))\n",
    "    attributes = np.array(dataset['attributes'])\n",
    "    data = np.array(dataset['data'])\n",
    "    return data, attributes\n",
    "\n",
    "# Use this to save bandwidth and time if the project has the data file in the /data folder\n",
    "def load_game_data_from_file():\n",
    "    file = 'data/pc_game_dataset.arff'\n",
    "    dataset = arff.load(open(file, 'r'))\n",
    "    attributes = np.array(dataset['attributes'])\n",
    "    data = np.array(dataset['data'])\n",
    "    return data, attributes\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2eaac541-a89c-44bb-96a0-f99c3ade4d0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "data, attributes = load_game_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fcc71a4e-3ec2-415e-83ec-b73c2bde5532",
   "metadata": {},
   "source": [
    "## Preprocessing\n",
    "This code digests the dataset into the form we need and prepares it for use by the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1007fdb-de28-4ac1-b120-fb3736ecdfe3",
   "metadata": {},
   "outputs": [],
   "source": [
    "stopwords = ['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59333629-8ecd-4f76-930b-d91d59743047",
   "metadata": {},
   "outputs": [],
   "source": [
    "import string\n",
    "\n",
    "def process_string(subject):\n",
    "    term = subject.strip()\n",
    "    term = str.lower(term)\n",
    "    term = term.translate(str.maketrans(\"\",\"\", string.punctuation))\n",
    "    return term"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6db8ec44-525b-4f46-bc7f-d989fb2887ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = set([])\n",
    "genres = data[:,6]\n",
    "for entry in genres:\n",
    "    terms = str(entry).split(',')\n",
    "    for term in terms:\n",
    "        results.add(process_string(term))\n",
    "y_headers = list(results)\n",
    "\n",
    "y = []\n",
    "for entry in data:\n",
    "    y_row = [0] * len(y_headers)\n",
    "    for genre in str(entry[6]).split(','):\n",
    "        y_row[y_headers.index(process_string(genre))] = 1\n",
    "    y.append(y_row)\n",
    "    \n",
    "y = np.array(y)\n",
    "\n",
    "# y is now our label vector\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "439dd05c-60c2-4ba2-8d8b-8a06652f84af",
   "metadata": {},
   "outputs": [],
   "source": [
    "bag = {}\n",
    "descriptions = data[:,25]\n",
    "\n",
    "for entry in descriptions:\n",
    "    terms = str(entry).split()\n",
    "    for term in terms:\n",
    "        term = process_string(term)\n",
    "        if term not in stopwords:  \n",
    "            if term in bag:\n",
    "                bag[term] = bag[term] + 1\n",
    "            else:\n",
    "                bag[term] = 1\n",
    "          \n",
    "# x_headers = list(bag)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d5b1bf3b-99c5-4da0-adba-dd6d5b187ec6",
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_bag = sorted(bag, reverse=True, key=bag.get)\n",
    "x_headers = sorted_bag[:20000]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "237d264b-43f2-4b56-9ac8-ead80b695b2f",
   "metadata": {},
   "source": [
    "This next cell takes almost ten minutes to run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2b0f8ec3-9374-483d-949c-8547516637ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[3 0 0 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "X = []\n",
    "for entry in data:\n",
    "    x_row = [0] * len(x_headers)\n",
    "    for word in str(entry[25]).split():\n",
    "        word = process_string(word)\n",
    "        if word in x_headers:\n",
    "            x_row[x_headers.index(word)] += 1\n",
    "    X.append(x_row)\n",
    "\n",
    "X = np.array(X)\n",
    "print(X[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b4efda3c-5ee3-4948-8d40-8a2bdd038e8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X is now our feature vector"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6be774e-e8c0-48bb-90a9-5cbca9322769",
   "metadata": {},
   "source": [
    "## Multi-Class SVC\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
